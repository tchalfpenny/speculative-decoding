# Speculative Decoding

### Todo:
- [ ] try different model families (gemini, llama, qwen)
- [ ] KV caching for faster inference

### Sources:
[arXiv: Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192)

[PyTorch: A Hitchhikerâ€™s Guide to Speculative Decoding](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)

[Hugging Face: Faster Text Generation with Self-Speculative Decoding](https://huggingface.co/blog/layerskip)

[Jay Gala: Speculative Decoding: From Theory to Implementation](https://galacodes.hashnode.dev/speculative-decoding#heading-putting-it-all-together)